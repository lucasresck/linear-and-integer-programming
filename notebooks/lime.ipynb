{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprising-values",
   "metadata": {},
   "source": [
    "# Mixed-Integer Linear Programming (MILP) for Local Interpretable Model-agnostic Explanations (LIME)\n",
    "\n",
    "This study aims to formulate (and test) Local Interpretable Model-agnostic Explanations (LIME) using Mixed-Integer Linear Programming (MILP).\n",
    "\n",
    "- Lucas Emanuel Resck Domingues\n",
    "- Professor: Luciano Guimar√£es\n",
    "- FGV-EMAp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-efficiency",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Optimization field is today one of the most important fields of Applied Mathematics.\n",
    "Not only the theory and the research are very solid, but also the applications are everywhere, from Engineering to Management. In general, problems of this kind try to deal with the maximization or minimization of a function, given some conditions in the variables.\n",
    "\n",
    "The main problem of Machine Learning, on the other hand, is to search for a function that represents the observed phenomenon using observed data. In fact, it can also be seen as an optimization problem, where the optimal function (given the data) is sought. For example, the goal of training a deep neural network is to search for a local mininum (with luck, a global mininum) for the loss function, given the data.\n",
    "\n",
    "Machine Learning models have a problem: many of them are not interpretable. Some of them are so incognito that they are called \"black-box\", in reference to airplane's black-box.\n",
    "This way, many researchers presented methods of explaining and interpreting Machine Learning model results.\n",
    "One of them is Local Interpretable Model-agnostic Explanations (LIME), the focus of this study.\n",
    "\n",
    "Suppose you have a text classification model (each text is assigned to a class) that says you if a text is mathematical or not. You want to understand why the model says this Introduction text (we call it $x$) is mathematical. LIME disturbs the text $x$ and verify the changes in probabilities $f(x)$. It tries to fit a simpler (but interpretable) model $g$, trying to predict the probabilities $f(x)$ over the pertubations of the input $x$. Mathematically, LIME tries to search for\n",
    "\n",
    "$$\\xi(x) = \\mathrm{argmin}_{g \\in G} \\mathcal{L}(f, g, \\pi_x) + \\Omega(g),$$\n",
    "\n",
    "being $G$ the set of desired interpretable functions, $\\pi_x$ the locallity around $x$, $\\mathcal{L}$ some measure of non proximity between $f, g$ in the locallity given by $\\pi_x$, and $\\Omega(g)$ being the complexity of $g$.\n",
    "$\\xi(x)$ will be a model that explains the prediction locally, but it will be interpretable too.\n",
    "\n",
    "In general, LIME formulation is not MILP, but we can addapt the formulation to have it MILP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-leone",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Only Python and Jupyter stuff. You can jump over this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "preliminary-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from pulp import LpVariable, LpProblem, value, LpStatus, LpMinimize\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "secondary-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-cross",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In this section, we will fit a text classification model to predict if a **movie comment is positive or negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-teaching",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data we use is the famous **IMDB dataset**, a set of movie comments classified as **positive** or **negative**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "heated-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    '''Preprocess text.'''\n",
    "    return text.replace('<br />', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "known-pledge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was it\n",
      "\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/IMDB Dataset.csv')\n",
    "df.review = df.review.apply(preprocessing)\n",
    "print(df.iloc[0, 0][:200])\n",
    "print()\n",
    "print(df.iloc[0, 1][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-maker",
   "metadata": {},
   "source": [
    "There is an example above of such a movie comment.\n",
    "\n",
    "Now, we split the data between training and test for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "binary-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.review.to_list()\n",
    "y = df.sentiment.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spatial-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-google",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "We fit and evaluate our text classification model (classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-float",
   "metadata": {},
   "source": [
    "First, we need to convert the texts to vectors.\n",
    "We do this using **TF-IDF**, a technique that create a vector for a text counting the number of times each word appears in the text (**TF**), but also normalizing by the number of times this word appears in all other texts (**IDF**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "veterinary-injection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 94342)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(\n",
    "    strip_accents=None,\n",
    "    lowercase=True,\n",
    "    smooth_idf=True,\n",
    ")\n",
    "X_train = tf_idf.fit_transform(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-craft",
   "metadata": {},
   "source": [
    "As we can see, the dimensionality of these vectors is huge: 94342.\n",
    "So we apply a **dimensionality reduction technique**, in our case Truncated SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "damaged-history",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "X_train = t_svd.fit_transform(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-small",
   "metadata": {},
   "source": [
    "The vectors have their coordinates **\"standardized\"**, that is, they are transformed to have mean zero and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "quantitative-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-yugoslavia",
   "metadata": {},
   "source": [
    "The classification model used is **Support Vector Machines (SVM)**.\n",
    "SVM tries to fit a hyperplane in the high-dimensional space where the vectors lie, trying to separate the vectors of different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "unlimited-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LinearSVC(\n",
    "    dual=False,  # n_samples > n_features\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# We do a grid search to search for the best hyperparameters\n",
    "svm = GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid={'C': np.logspace(-2, 10, 13)},\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "rubber-poetry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]CPU times: user 4.65 s, sys: 2.25 s, total: 6.9 s\n",
      "Wall time: 7.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit SVM\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Fit probabilities\n",
    "svm = CalibratedClassifierCV(svm.best_estimator_, cv=5)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "vector = Pipeline([\n",
    "    ('tf_idf', tf_idf),\n",
    "    ('t_svd', t_svd)\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('tf_idf', tf_idf),\n",
    "    ('t_svd', t_svd),\n",
    "    ('scaler', scaler),\n",
    "    ('svm', svm)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-worth",
   "metadata": {},
   "source": [
    "We evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "demonstrated-dominican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8427"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-wrong",
   "metadata": {},
   "source": [
    "We see that SVM predicts 84\\% of data right!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-purse",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Now we enter the optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-design",
   "metadata": {},
   "source": [
    "### LIME\n",
    "\n",
    "#### Original formulation\n",
    "\n",
    "The formal original formulation of LIME is as follows.\n",
    "\n",
    "Let $x \\in \\mathbb{R}^d$ be the **representation** of an instance (the vector of a text). We define $x' \\in \\{0, 1\\}^{d'}$ the **interpretable representation** of $x$ (in our case, a binary vector indicating which words are the in text).\n",
    "$G$ is a set of **interpretable models** $g \\colon \\{0, 1\\}^{d'} \\to \\mathbb{R}$, and $\\Omega(g)$ is defined as the complexity of a model $g \\in G$.\n",
    "\n",
    "Be $f \\colon \\mathbb{R}^d \\to \\mathbb{R}$ the model we want to explain, for example, the probability of a text $x$ being a negative movie review. $\\pi_x(z)$ is some measure of locallity of $z$ around $x$ (it is greater if both vectors are closer, for example). $\\mathcal{L}(f, g, \\pi_x)$ is defined as the measure of non approximation between $f$ and $g$ in the locallity of $x$ given by $\\pi_x$.\n",
    "\n",
    "Given such definitions, the goal of LIME is to search for an interpretable model $g \\in G$ such that\n",
    "\n",
    "$$\\xi(x) = \\mathrm{argmin}_{g \\in G} \\mathcal{L}(f, g, \\pi_x) + \\Omega(g),$$\n",
    "\n",
    "Both $\\pi_x$ and $\\mathcal{L}$ deal with locallity around $x$.\n",
    "It is achieved disturbing the vector $x' \\in \\{0, 1\\}^{d'}$, turning some coordinates into 0 (*i.e.*, erasing some words), resulting in various pertubations $z'$. From these interpretable representations we restore the representations $z$.\n",
    "\n",
    "The very own LIME paper also sets the path for text classification models.\n",
    "They define the set $G$ of interpretable models as the set of linear models $g(z') = w_g \\cdot z'$.\n",
    "The measure of non approximation is the locally weighted square loss\n",
    "\n",
    "$$\\mathcal{L}(f, g, \\pi_x) = \\sum_{z, z'} \\pi_x(z) \\left(f(z) - g(z')\\right)^2$$\n",
    "\n",
    "That is, the linear model $g(z')$ is trying to predict the probability $f(z)$.\n",
    "They also define the complexity $\\Omega$ as\n",
    "\n",
    "$$\\Omega(g) = \\infty \\cdot \\mathbb{1}(\\lVert w_g \\rVert_0 > K),$$\n",
    "\n",
    "that is, it is zero if $K$ or less components are not zero, and $\\infty$ if more than $K$ components are not zero.\n",
    "\n",
    "The locallity $\\pi_x(z)$ is given by $\\pi_x(z) = \\exp\\left(-D(x, y)^2/\\sigma^2\\right)$, with $D$ being the cosine distance between $x, y$, that is, $D(x, z) = \\frac{1}{\\pi} \\arccos \\frac{x \\cdot z}{\\lVert x \\rVert \\lVert z \\rVert}$.\n",
    "\n",
    "#### MILP formulation\n",
    "\n",
    "The whole formulation above tries to search for a linear model over the perturbation of $x$ trying to predict the probabilities given by $x$.\n",
    "The coefficients of the linear model will give us the importance of each word.\n",
    "\n",
    "The idea is to have everything calculated, and to just optimize the coefficients of the linear model $g$.\n",
    "This way, using the original formulation, we end up with an optimization problem that is not MILP.\n",
    "See that the $g$ model in $\\mathcal{L}$ is inside a square.\n",
    "We also have a problem with $\\Omega(g)$, that is intractable, according to the paper.\n",
    "We will have to relax some of these.\n",
    "\n",
    "A possible solution is as follows:\n",
    "\n",
    "$$\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        &\\underset{g \\in G}{\\text{Minimize}}&&\\mathcal{L}(f, g, \\pi_x) + \\lambda \\Omega(g)\\\\\n",
    "        &\\text{subject to} &&\\mathcal{L}(f, g, \\pi_x) = \\sum_{z, z'} \\pi_x(z) \\lvert f(z) - g(z') \\rvert,\\\\\n",
    "        &&&\\Omega(g) = |w_g|_1,\\\\\n",
    "        &&&g(z') = w_g \\cdot z', \\ \\ \\forall z'\\\\\n",
    "        &&&\\mathcal{L} \\in \\mathbb{R},\\\\\n",
    "        &&&\\Omega \\in \\mathbb{R},\\\\\n",
    "        &&&g(z') \\in \\mathbb{R}, \\ \\ \\forall z'\\\\\n",
    "        &&&w_g \\in \\{0, 1\\}^{d'}\\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "(Remember that $\\pi_x(z)$, $f(z)$ and $z'$ are already calculated.)\n",
    "\n",
    "We modify the formulation to have it MILP:\n",
    "\n",
    "$$\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        &\\underset{g \\in G}{\\text{Minimize}}&&\\mathcal{L} + \\lambda \\Omega\\\\\n",
    "        &\\text{subject to} &&\\mathcal{L} = \\sum_{i} \\pi_x(z_i) \\varepsilon_i,\\\\\n",
    "        &&&-\\varepsilon_i \\le f(z_i) - g_i \\le \\varepsilon_i, \\ \\ \\forall i\\\\\n",
    "        &&&\\Omega = \\sum_j \\delta_j,\\\\\n",
    "        &&&-\\delta_j \\le x_j \\le \\delta_j, \\ \\ \\forall j\\\\\n",
    "        &&&g_i = \\sum_j z_{ij}' x_j, \\ \\ \\forall i\\\\\n",
    "        &&&\\mathcal{L} \\in \\mathbb{R},\\\\\n",
    "        &&&\\Omega \\in \\mathbb{R},\\\\\n",
    "        &&&g_i \\in \\mathbb{R}, \\ \\ \\forall i\\\\\n",
    "        &&&x_j \\in \\{0, 1\\}, \\ \\ \\forall j\\\\\n",
    "        &&&\\varepsilon_i \\ge 0, \\ \\ \\forall i\\\\\n",
    "        &&&\\delta_j \\ge 0, \\ \\ \\forall j\\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "In fact, it is a weighted linear regression with LASSO regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-failure",
   "metadata": {},
   "source": [
    "### Calculation of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(text):\n",
    "    '''Probability of a text'''\n",
    "    return model.predict_proba([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi(x, z, sigma_2=-1/np.log(0.1)):\n",
    "    '''Weights of locallity.'''\n",
    "    x = vector.transform([x])[0]\n",
    "    z = vector.transform([z])[0]\n",
    "    # If null vector\n",
    "    if np.abs(z).sum() == 0:\n",
    "        return 0\n",
    "    # Cosine of angle between vectors\n",
    "    cos = np.dot(x, z)/(np.linalg.norm(x)*np.linalg.norm(z))\n",
    "    # If cosine is like 1.00008\n",
    "    if cos > 1:\n",
    "        cos = 1\n",
    "    # Angle between vectors, normalized to between 0 and 1\n",
    "    D = np.arccos(cos)*2/np.pi\n",
    "    return np.exp(-D**2/sigma_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(split, which_class, M, N, K):\n",
    "    '''Return the parameters for LIME optimization.'''\n",
    "    # Perturbations\n",
    "    z_line = []\n",
    "    # Probabilities\n",
    "    f_z = []\n",
    "    # Weights\n",
    "    pi_x = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Choose a random number of words to remove, between 1 and (split - 1)\n",
    "        n = np.random.choice(range(1, M))\n",
    "        # Remove n random words\n",
    "        indices = np.random.choice(range(M), size=n, replace=False)\n",
    "        \n",
    "        # The pertubartion\n",
    "        perturbation = np.ones(M)\n",
    "        for index in indices:\n",
    "            perturbation[index] = 0\n",
    "        z_line.append(perturbation)\n",
    "            \n",
    "        # The probability and weight\n",
    "        text = ' '.join([word for (j, word) in enumerate(split) if perturbation[j]])\n",
    "        f_z.append(f(text)[which_class])\n",
    "        pi_x.append(pi(example, text))\n",
    "    \n",
    "    return z_line, f_z, pi_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-transcription",
   "metadata": {},
   "source": [
    "### Linear optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(z_line, f_z, pi_x, M, N, K, lambda_=0.5):\n",
    "    '''The MILP for LIME.'''\n",
    "    prob = LpProblem(\"LIME\", LpMinimize)\n",
    "    \n",
    "    # Variables\n",
    "    L = LpVariable('L')\n",
    "    Omega = LpVariable('Omega')\n",
    "    epsilon = [LpVariable('epsilon_{}'.format(i)) for i in range(N)]\n",
    "    delta = [LpVariable('delta_{}'.format(i)) for i in range(M)]\n",
    "    g = [LpVariable(\"g(z'_{})\".format(i)) for i in range(N)]\n",
    "    x = [LpVariable('x_{}'.format(j)) for j in range(M)]\n",
    "#     y = [LpVariable('y_{}'.format(j), 0, 1, cat='Integer') for j in range(M)]\n",
    "    \n",
    "    # Objective\n",
    "    prob += L + lambda_*Omega\n",
    "    \n",
    "    # Constraints\n",
    "    prob += L == sum([pi_x[i]*epsilon[i] for i in range(N)])\n",
    "    prob += Omega == sum(delta)\n",
    "\n",
    "    for i in range(N):\n",
    "        prob += -epsilon[i] <= f_z[i] - g[i]\n",
    "        prob += epsilon[i] >= f_z[i] - g[i]\n",
    "        prob += g[i] == sum([z_line[i][j]*x[j] for j in range(M)])\n",
    "\n",
    "    infinity = 100000\n",
    "    for j in range(M):\n",
    "        prob += -delta[j] <= x[j]\n",
    "        prob += delta[j] >= x[j]\n",
    "        \n",
    "#         prob += -infinity*y[j] <= x[j]\n",
    "#         prob += infinity*y[j] >= x[j]\n",
    "\n",
    "#     prob += sum(y) <= K\n",
    "\n",
    "    print('Solving MILP...')\n",
    "    status = prob.solve()\n",
    "    print('Done.')\n",
    "    \n",
    "    return prob, status, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(split, importances):\n",
    "    '''Visualize the importance of each word in the classification.'''\n",
    "    max_abs_importance = np.max(np.abs(importances))\n",
    "    # Green\n",
    "    positive = np.array([0, 255, 0])\n",
    "    white = np.array([255, 255, 255])\n",
    "    # Red\n",
    "    negative = np.array([255, 0, 0])\n",
    "    spans = []\n",
    "    for i, word in enumerate(split):\n",
    "        if importances[i] >= 0:\n",
    "            color = white + (positive - white)/max_abs_importance*importances[i]\n",
    "        else:\n",
    "            color = white + (negative - white)/((-1)*max_abs_importance)*importances[i]\n",
    "        spans.append(\n",
    "            '<span style=\"background-color: RGB({R}, {G}, {B})\">{word}</span>'.format(\n",
    "                word=word,\n",
    "                R=color[0],\n",
    "                G=color[1],\n",
    "                B=color[2]\n",
    "            )\n",
    "        )\n",
    "            \n",
    "    html = ' '.join(spans)\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime(text, which_class, N=None, K=None):\n",
    "    # Split \n",
    "    split = text.split()\n",
    "    M = len(split)\n",
    "    if N is None:\n",
    "        N = 5*M\n",
    "#     if K is None:\n",
    "#         K = min([M, 20])\n",
    "                \n",
    "    z_line, f_z, pi_x = parameters(split, which_class, M, N, K)\n",
    "        \n",
    "    prob, status, x = optimization(z_line, f_z, pi_x, M, N, K)\n",
    "    \n",
    "    importances = [value(i) for i in x]    \n",
    "    print(dict(zip(split, importances)))    \n",
    "    \n",
    "    return visualize(split, importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-trustee",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'This movie is awful, I regret seing it, it is a bad movie.'\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba([example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime(example, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-provider",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://arxiv.org/pdf/1602.04938.pdf\n",
    "- https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "- https://vanderbei.princeton.edu/tex/talks/MOPTA14/L1_reg.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
